{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_Word2Vec.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"z-D63ddOQRmW"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"rumBGbZB_3Zx"},"source":["<center><h1>Introduction to Word2Vec</center> "]},{"cell_type":"markdown","metadata":{"id":"gJ558yRMwpUd"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**Word2Vec Architecture**](#section1)<br>\n"," - **1.1.** [**Continuous Bag of Words**](#section11)<br>\n"," - **1.2.** [**Skip Gram**](#section12)\n","\n","**2.** [**Generating Word Vectors using Word2Vec**](#section2)<br>\n","  - **2.1** [**Importing necessary Libraries**](#section21)\n","  - **2.2** [**Importing the data**](#section22)\n","  - **2.3** [**Iterating through each sentence through the file**](#section23)\n","  - **2.4** [**Creating a CBOW model**](#section24)\n","  - **2.5** [**Creating a Skip Gram model**](#section25)\n","  - **2.6** [**Output**](#section26)\n","\n","**3.** [**Applcations**](#section3)<br>"]},{"cell_type":"markdown","metadata":{"id":"0ZRZAk9PXQ4N"},"source":["---\n","<a name = Section1></a>\n","# **1. Word2Vec Architectures**\n","---"]},{"cell_type":"markdown","metadata":{"id":"Vof5FSa6w2BZ"},"source":["- **Word2Vec** is an algorithm for constructing **vector** representations of words, also known as **word embeddings**.\n","\n","\n","- **Word2Vec** takes care of 2 things:\n","\n","  - It converts **high dimensional** vector into **low** dimensional vector.\n","  \n","  - Maintains the **word context**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QLCiN4AlQRmd"},"source":["- **Word2Vec** is one of the most widely used **models** to produce **word embeddings**.\n","\n","- The models are **shallow**, 2 layer **neural** networks that are trained to reconstruct **linguistic** **context** of the word.\n"]},{"cell_type":"markdown","metadata":{"id":"IegBB4aAQRmg"},"source":["- **Layers --> Input layer + Hidden layer = Output layer**\n","\n","\n","<br>  \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/hidden.PNG\"width='450' height='280'/></center>"]},{"cell_type":"markdown","metadata":{"id":"1cwxQOoEX_O6"},"source":["<a id=section1.1></a>\n","### **1.1 CBOW (Continuous Bag of Words)**"]},{"cell_type":"markdown","metadata":{"id":"cGyalx5jQRmi"},"source":["- It learns to **predict** the word by context.\n","\n","- Input  --> the context (neighboring words)\n","\n","- Output --> target word\n","\n","The **limit** on the number of words in each context is **determined** by a parameter called **`window size`**.\n","\n","<br>  \n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/cbow.png\" width='600' height='380'/></center>\n","<br>  "]},{"cell_type":"markdown","metadata":{"id":"uUvhgZTJZDZ3"},"source":["<a id=section201></a>\n","### **1.2 SKIP Gram**"]},{"cell_type":"markdown","metadata":{"id":"fnV-gAy7AgSZ"},"source":["- Skip Gram is **learning** to predict the **context** by the word.\n","\n","- Input  --> Word\n","\n","- Output --> Target Context (neighboring words)\n","\n","- The limit on the number of words in **each** context is **determined** by a **parameter** called **`window size`**.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/SKIP_gram.png\" width='650' height='380'/></center>\n","<br>  \n","\n","- In the above example:\n","\n","  - **INPUT Layer**  : Blue box content\n","\n","  - **TARGET Layer** : White box content\n","\n","  - **Window Size**  : 5\n"]},{"cell_type":"markdown","metadata":{"id":"kRrvTuHJQRml","toc-hr-collapsed":true},"source":["#### A simple example in Python to generate word vectors using Word2Vec\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/python.PNG\" width='580' height='300'/></center>"]},{"cell_type":"markdown","metadata":{"id":"O8Bz3LmSQRmo"},"source":["- Run these 2 commands in the terminal :\n","\n","```python\n","pip install nltk\n","pip install gensim\n","```"]},{"cell_type":"markdown","metadata":{"id":"b9r8rXDeA1gI"},"source":["---\n","<a name = Section2></a>\n","# **2. Generating Word Vectors using Word2Vec**\n","---"]},{"cell_type":"markdown","metadata":{"id":"vVN-l4YyQRmr"},"source":["<a id=section201></a>\n","### **2.1 Importing necessary libraries**"]},{"cell_type":"code","metadata":{"id":"Sq-Nr5uFQRmt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e553b9c7-6ebe-42bb-b6ea-c2f3103f6587"},"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","import warnings \n","import gensim \n","from gensim.models import Word2Vec\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"yITVp8ueQRm4"},"source":["warnings.filterwarnings(action = 'ignore') # This is to ignore unnecessary warnings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kY93wHs7QRm9"},"source":["<a id=section202></a>\n","### **2.2 Importing the data** \n"]},{"cell_type":"code","metadata":{"id":"wA_xzRmRbjvu"},"source":["import urllib\n","sample = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/Alice.txt')\n","s = sample.read().decode('utf8')\n","f = s.replace(\"\\n\", \" \")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sW6OpLbIQRnH"},"source":["<a id=section203></a>\n","### **2.3 Iterating through each sentence in the file**"]},{"cell_type":"code","metadata":{"id":"1eErGUe7QRnK"},"source":["data = []\n","\n","for i in sent_tokenize(f): \n","    temp = [] \n","      \n","    # tokenize the sentence into words \n","    for j in word_tokenize(i): \n","        temp.append(j.lower()) \n","  \n","    data.append(temp) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6w8OWPtQRnS"},"source":["\n","<a id=section204></a>\n","### **2.4 Creating a CBOW model**"]},{"cell_type":"code","metadata":{"id":"qs2IAamIQRnV","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e64410d-2c91-4cea-e1c7-47c51b3cd8b2"},"source":["model1 = gensim.models.Word2Vec(data, min_count = 1,  \n","                              size = 100, window = 5) \n","\n","# Print results \n","print(\"Cosine similarity between 'alice' \" + \n","               \"and 'wonderland' - CBOW : \", \n","    model1.similarity('alice', 'wonderland')) \n","      \n","print(\"Cosine similarity between 'alice' \" +\n","                 \"and 'machines' - CBOW : \", \n","      model1.similarity('alice', 'machines')) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9994259\n","Cosine similarity between 'alice' and 'machines' - CBOW :  0.9537079\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rH_x3fG7QRng"},"source":["<a id=section205></a>\n","### **2.5 Creating a Skip Gram model**"]},{"cell_type":"code","metadata":{"id":"F5t1tLC1QRnh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1771f6b5-4bea-48b0-a623-d20fa4b208d7"},"source":["model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n","                                             window = 5, sg = 1) \n","  \n","# Print results \n","print(\"Cosine similarity between 'alice' \" +\n","          \"and 'wonderland' - Skip Gram : \", \n","    model2.similarity('alice', 'wonderland')) \n","      \n","print(\"Cosine similarity between 'alice' \" +\n","            \"and 'machines' - Skip Gram : \", \n","      model2.similarity('alice', 'machines')) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8895068\n","Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.84547246\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hfKtdydmYOyk"},"source":["<a id=section201></a>\n","### **2.6 Output**"]},{"cell_type":"markdown","metadata":{"id":"5wzTkQZxQRns"},"source":["- Cosine similarity between 'alice' and 'wonderland' - CBOW :  **0.99913293**\n","\n","- Cosine similarity between 'alice' and 'machines' - CBOW :  **0.98022455**\n","\n","- Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  **0.89401996**\n","\n","- Cosine similarity between 'alice' and 'machines' - Skip Gram :  **0.85758847**\n","\n","- Output indicates the **cosine similarities** between word vectors **‘alice’, ‘wonderland’** and **‘machines’** for **different** models."]},{"cell_type":"markdown","metadata":{"id":"KtOEilfQXZCp"},"source":["---\n","<a name = Section3></a>\n","# **3. Applications of Word Embeddings**\n","---"]},{"cell_type":"markdown","metadata":{"id":"SSxdeRq9QRnv"},"source":["\n","- **Sentiment Analysis**\n","\n","- **Speech Recognition**\n","\n","- **Information Retrieval**\n","\n","- **Question Answering**"]}]}