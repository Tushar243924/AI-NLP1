{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP1_Module1_Assignment_Questions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UcgNSIvuIQqK"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>\n","\n","**<center><h3>NLP 1 Module 1 Assignment Questions</h3></center>**"]},{"cell_type":"markdown","metadata":{"id":"_ATbV26DaIt4"},"source":["----\n","# **Table of Contents**\n","----\n","\n","**1.** [**Problem Statement**](#section1)<br> \n","**2.** [**Import Libraries**](#section2)<br> \n","**3.** [**Data Description**](#section3)<br> \n","**4.** [**Text Preprocessing**](#section4)<br>\n","   - **4.1** [**Lowercasing**](#section401)\n","   - **4.2** [**Word Tokenisation**](#section402)\n","   - **4.3** [**Stopwords Removal**](#section403)\n","   - **4.4** [**Stemming**](#section404)\n","   - **4.5** [**Lemmantization**](#section405)\n","   - **4.6** [**Part of Speech Tagging**](#section406)\n","\n","**5.** [**Name Entity Recognition**](#section5)<br>\n","**6.** [**Conclusion**](#section6)<br>"]},{"cell_type":"markdown","metadata":{"id":"BmlyYG19ckwb"},"source":["---\n","<a id=section1></a>\n","# **1. Problem Statement**\n","----"]},{"cell_type":"markdown","metadata":{"id":"PivlYqm-lni5"},"source":["- To apply serval **Text Preprocessing** techniques on **text corpus**.\n","\n","- In order to use **ML techniques** on text data we need to convert **text** data to **numerical** values. For that we need to **apply** various text **preprocessing** techniques. \n","\n","- **Preprocess** your text simply **means** to bring your text into a form that is **predictable** and **analyzable** for your task."]},{"cell_type":"markdown","metadata":{"id":"e2bYQTRxcqug"},"source":["----\n","<a id=section2></a>\n","# **2. Import Libraries**\n","----"]},{"cell_type":"code","metadata":{"id":"JEbgByRkpQab"},"source":["import nltk\n","from nltk.corpus import brown\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk import sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import SnowballStemmer\n","from nltk import FreqDist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AuTNV32pQad"},"source":["import warnings\n","warnings.filterwarnings(action = 'ignore')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2ZnpzYccuLE"},"source":["----\n","<a id=section3></a>\n","# **3. Data Description**\n","----"]},{"cell_type":"markdown","metadata":{"id":"VaTZnCyPiK1U"},"source":["- We are using **Brown Corpra** for our problem.\n","\n","- The **corpus** consists of **one million words** of **American English** texts printed in **1961**.\n","\n","- This corpus contains text from **500** sources, and the sources have been **categorised** by genre, such as **news**, **editorial**, and so on.\n","\n","**Example document for each section of the Brown Corpus**: \n","\n","<br>  \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/Sqweeks%20image/text_corpra_brown.png\" width=\"1000\" height=\"400\" /></center>\n","\n","<br>  "]},{"cell_type":"markdown","metadata":{"id":"HGdPWm-_pQaf"},"source":["**<h4>Question 1:** How we can download the Brown corpus using NLTK?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use **`nltk.download()`** to download any corpus.\n","\n","- Give `brown` as parameter.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"peRT9QBdpQag"},"source":["def download_brown():\n","    # Write code to download the brown corpus here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwRJtJZ2pQag"},"source":["download_brown()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93FwOKYqUGZY"},"source":["- Printing **categories** in **text corpus**:"]},{"cell_type":"code","metadata":{"id":"4FZGuCCApQah"},"source":["def print_brown_categories():\n","    print(brown.categories())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1oaubG1pQah"},"source":["print_brown_categories()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhoZQlCycwzQ"},"source":["----\n","<a id=section4></a>\n","# **4. Text Preprocessing**\n","----"]},{"cell_type":"markdown","metadata":{"id":"7bX9f6yGpQLU"},"source":[" \n","- We know that in order to deal with **textual data** with **Machine Learning** we need to **convert** it to **numeric** form.\n","\n","- But before applying **embedding**(to convert text to numeric form) **text data** should be **clean** and **preprocessed**.\n","\n","-  The **pre-processing** steps for a problem **depend** mainly on the **domain** and the **problem**.\n","\n","- The various text **preprocessing** steps are:\n","  - **Lower Casing Text**\n","  - **Tokenization**\n","  - **Removal of Punctuations**\n","  - **Removal of Stopwords**\n","  - **Removal of Frequent words**\n","  - **Stemming**\n","  - **Lemmatization**\n","  - **Removal of Emojis**\n","  - **Conversion of Emojis to words**\n","  - **Removal of URLs**\n","  - **Spelling correction**"]},{"cell_type":"markdown","metadata":{"id":"ioRG5OD5XsyU"},"source":["<a id=section401></a>\n","### **4.1 Lowercasing**"]},{"cell_type":"markdown","metadata":{"id":"u2UPgzhkd8BA"},"source":["- Words like \"**INSAID**\", \"**Insaid**\" and \"**insaid**\" means same.\n","\n","- But without **lower case** we need **3** word vector to represent them in **vector space**.\n","\n","- It will also helps in **reducing** size of text **data vocabulary**.\n"]},{"cell_type":"code","metadata":{"id":"Skeve-33pQaj"},"source":["text =  \"The capital of India is New Delhi.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPZgKgnypQaj"},"source":["**<h4>Question 2:** How we can convert our Text to lower case?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use **`text.lower()`** to lowercase text where text is input word/sentence  string.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"DXN8emtSpQaj"},"source":["def to_lower(text):\n","    lower_text = # Write code to convert text to lowercase here.\n","\n","    return lower_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qg31YlhppQaj"},"source":["lower_text = to_lower(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiuH18oepQak"},"source":["print(lower_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BWIURiM5fyrj"},"source":["\n","<a id=section401></a>\n","### **4.2 Word Tokenisation**"]},{"cell_type":"markdown","metadata":{"id":"LmTtmvkof4XI"},"source":["- Tokenization is the process of **splitting** the given text into smaller pieces called **tokens**.\n","\n","- Words, numbers, punctuation marks, and others can be considered as tokens.\n","\n","- For example if input sentence is \"**International School of AI and Data Science**\" so after **tokenisation** output will be **`[International, School, of, AI, and, Data, Science]`**\n"]},{"cell_type":"code","metadata":{"id":"zf8j6KYapQal"},"source":["text =  \"The capital of India is New Delhi.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GoPCxWt3pQal"},"source":["**<h4>Question 3:** How to perform Word Tokenisation on a sentence?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Apply **`nltk.word_tokenize()`** on `text` variable to tokenize text.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"XBplxICnpQal"},"source":["def tokenize_fn(text):\n","    word_tokens = # Write code to perform word tokenization here.\n","\n","    return word_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaWr4i_npQam"},"source":["word_tokens = tokenize_fn(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MamAGhtzpQam"},"source":["print(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMEjbpDPpQam"},"source":["text =  \"The capital of India is New Delhi. The capital of China is Beijing.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nhku1BMcpQam"},"source":["**<h4>Question 4:** What if we want to tokenize sentences from paragraph?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Apply Sentence Tokenizer as **`nltk.sent_tokenize()`** on `text` variable.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"EgRNa3PGpQan"},"source":["def tokenize_fn(text):\n","    sent_tokens = # Write code to perform sentence tokenization here.\n","\n","    return sent_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Asxd9YfpQan"},"source":["sent_tokens = tokenize_fn(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjKKy8PypQan"},"source":["print(sent_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qKfIME6jvYP"},"source":["\n","<a id=section401></a>\n","### **4.3 Stopwords Removal**"]},{"cell_type":"markdown","metadata":{"id":"tnAULoLGjzdr"},"source":["- Stop words are a set of **commonly** used words in any language.\n","\n","- Stopwords usually don't provide **valuable** information for analysis.\n","\n","- The intuition behind not using **stop words** is that, by removing **low** information words from **text**, we can focus on the important words instead.\n","\n","- In English words like **\"i\", \"is\", \"was\", \"because\", \"him\"** are considered as **stopwords**.\n","\n","- For example if our input sentence is **`CEO is a cool guy`** then output after removing stopwords will be **`[CEO, cool, guy]`**."]},{"cell_type":"markdown","metadata":{"id":"6Dk416iNmCpF"},"source":["#### List of stopwords in English language:"]},{"cell_type":"code","metadata":{"id":"5Dext7B1mKDh"},"source":["\", \".join(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_gQzmm3LpQap"},"source":["stopword = stopwords.words(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6ZDL34npQap"},"source":["text =  \"The capital of India is New Delhi.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwgN_teapQaq"},"source":["word_tokens = nltk.word_tokenize(text)\n","print(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amcWFxQspQaq"},"source":["**<h4>Question 5:** How to remove stopwords from the text?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use **list comprehension** to remove words from `word_tokens` that are also in stopword list.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"5IfoDm3HpQaq"},"source":["def stopword_removal(word_tokens):\n","    removing_stopwords = # Write code for stopword removal here.\n","\n","    return removing_stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5vwp_uRpQar"},"source":["removing_stopwords = stopword_removal(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfF2qWEupQar"},"source":["print(removing_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySknx-09m-oM"},"source":["<a id=section401></a>\n","### **4.4 Stemming**"]},{"cell_type":"markdown","metadata":{"id":"pjNSOlSQoysq"},"source":["- **Stemming** is the process of getting the **root form** of a word.\n","\n","- In stemming we **chops off** end of words to get their **root** form. \n","\n","- Root form can or can't be a **real word**. \n","\n","- Stemming is useful for dealing with **sparsity** issues as well as **standardizing** vocabulary.\n","\n","- For Example Words like **\"Connect\", \"Connects\", \"Connected\", \"Connections\",\"Connection\"** will be converted to **\"Connect\"**."]},{"cell_type":"code","metadata":{"id":"AADyOyqTpQar"},"source":["snowball_stemmer = SnowballStemmer(\"english\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ocX9xgVpQat"},"source":["text = \"The athletes are running faster.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fluem8DxpQat"},"source":["word_tokens = nltk.word_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06osTqSmpQat"},"source":["**<h4>Question 6:** How to perform Stemming on a given corpus?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use Snowball stemmer as **`snowball_stemmer.stem()`** to apply stemming on `word_tokens`.\n","\n","- Perform tasks using list comprehension. \n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"rMJLyHmHpQat"},"source":["def stemming_fn(word_tokens):\n","    stemmed_word = # Write code for performing stemming here.\n","\n","    return stemmed_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvSifXvIpQat"},"source":["stemmed_word = stemming_fn(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kz3mZgFUpQau"},"source":["print(stemmed_word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaTpp2nMoVhc"},"source":["\n","<a id=section401></a>\n","### **4.5 Lemmatization**"]},{"cell_type":"markdown","metadata":{"id":"gBI_jN8P2DFK"},"source":["- Lemmatization process also converts words to their **reduced** form.\n","\n","- But unlike **Stemming**, Lemmatization converts words to **pre-existing** words in any language.\n","\n","- **Converted words**(Root Words) are called **Lemma**.\n","\n","- For example words **\"Troubles\", \"Troubling\", \"Troubled\", \"Trouble\"** can be reduced to word **\"Trouble\"**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Kb3Uy1nvdhcF"},"source":["<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/Sqweeks%20image/c71d705d-445d-4d4c-99d1-38ce48985cba_12.JPG\" width=\"800\" height=\"350\" /></center>"]},{"cell_type":"code","metadata":{"id":"jE4TTCirn3kb"},"source":["nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_S7FUiYLn3ki"},"source":["text = \"The athletes are running faster.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YegSqfV5n3kh"},"source":["wordnet_lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SGXJxacn3kj"},"source":["word_tokens = nltk.word_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouAZt0Z18eEb"},"source":["word_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFmslvR0n3kY"},"source":["**<h4>Question 7:** How to perform perform Lemmatization on word corpus?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Apply Lemmatization using **`wordnet_lemmatizer.lemmatize()`** on `word_tokens`.\n","\n","- Perform task using **list comprehension** method.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"38n2F0WVn3kk"},"source":["def lemmatization_fn(word_tokens):\n","    lemmatized_word = # Write code for performing lemmatization here.\n","\n","    return lemmatized_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsukE9Qan3km"},"source":["lemmatized_word = lemmatization_fn(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2mRHAqQn3ko"},"source":["print(lemmatized_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vn1kwL6XpQax"},"source":["text = \"Twinkle, twinkle, little star, How I wonder what you are.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Gl0K3EpQax"},"source":["word_tokens = nltk.word_tokenize(text.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ly3A3YfFpQax"},"source":["print(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVlorA2ppQax"},"source":["**<h4>Question 8:** How to calculate the Word Frequency of each word in the text and print the n most common words?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Apply **`FreqDist()`** from NLTK on `word_tokens` variable .\n","\n","- Use **`freq.most_comman()`** fuction with **n=5** as **parameter**.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"eDp-8QdKpQax"},"source":["def word_frequency_fn(word_tokens):\n","    freq = # Write code to calculate the frequency distribution of words here.\n","    \n","    print(# Write code to print the 5 most common words here.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEs_BbbVpQax"},"source":["word_frequency_fn(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0akmIqCtFylC"},"source":["<a id=section401></a>\n","### **4.6 Part-of-Speech (POS) Tagging**"]},{"cell_type":"markdown","metadata":{"id":"E1NbiTnnF5z8"},"source":["\n","- **Part-of-speech**(POS) tagging is a process to **mark word** in a text **corpus** based on **definition** as well as its **context** and **sementic** **meaning**. \n","\n","- **Part-of-speech(POS)** tagging **aims** to assign parts of speech to each word of a given **text** **(such as nouns, verbs, adjectives, and others)**. \n","\n","- For example sentence **`\"the kids are playing in the park.\"`** will give output as **follows** after applying **POS**:\n","\n","**`[('the', 'DT'), ('kids', 'NNS'), ('are', 'VBP'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('park', 'NN'), ('.', '.')]`**\n","\n","#### POS tag list:\n","\n","\n","    CC\t  coordinating conjunction\n","    CD\t  cardinal digit\n","    DT\t  determiner\n","    IN\t  preposition/subordinating conjunction\n","    JJ\t  adjective\n","    JJR\t  adjective, comparative\n","    JJS\t  adjective, superlative\n","    MD\t  modal\t\n","    NN\t  noun, singular\n","    NNS\t  noun plural\n","    NNP\t  proper noun, singular\n","    NNPS\tproper noun, plural\n","    PDT \tpredeterminer\n","    POS \tpossessive ending\t\n","    PRP \tpersonal pronoun\n","    PRP$\tpossessive pronoun\n","    RB\t  adverb\n","    RBR  \tadverb, comparative\n","    RBS  \tadverb, superlative\n","    RP  \tparticl\n","    UH  \tinterjection\n","    VB  \tverb\n","    VBD  \tverb, past tense\n","    VBG \tverb, gerund/present participle\n","    VBN  \tverb, past participle\n","    VBP \tverb, present\n","    VBZ \tverb, 3rd person\n","    WDT \twh-determiner\n"]},{"cell_type":"code","metadata":{"id":"LBP2ELikpQay"},"source":["nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68cdj8FrpQay"},"source":["text = \"the kids are playing in the park.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvP_j-YgpQay"},"source":["word_tokens = nltk.word_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_0kafBrpQay"},"source":["**<h4>Question 9:** How to perform POS (Part of Speech) Tagging?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use **`nltk.pos_tag()`** with `word_tokens` as parameter to apply POS.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"GebaCNKfpQaz"},"source":["def pos_tagging(word_tokens):\n","    pos_tag = # Write code to perform part of speech tagging here.\n","\n","    return pos_tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ8UvAi5pQaz"},"source":["pos_tag = pos_tagging(word_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZoU96_hpQaz"},"source":["print(pos_tag)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRSkwxUSNENG"},"source":["----\n","<a id=section5></a>\n","# **5. Name Entity Recognition**\n","----"]},{"cell_type":"markdown","metadata":{"id":"afqPYC2lQL65"},"source":["- The aim of Name Entity Recognition is to **find** and **classify** named entities in text into pre-defined **categories** such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n","\n","- NER can be **useful** in applications like Classifying content for news providers, Summarizing Resumes, Optimizing Search Engine Algorithms, Simplifying Customer Support. \n"]},{"cell_type":"code","metadata":{"id":"MsEjhF7JpQa0"},"source":["text = \"New Delhi is the capital of India. Beijing is the capital of China.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rU4tVk9SpQa0"},"source":["# Tokenize the input text \n","word_tokens = nltk.word_tokenize(text)\n","word_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPyl5ZStpQa1"},"source":["#Applying POS Tagging\n","pos_tag = nltk.pos_tag(word_tokens)\n","pos_tag"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2WNykdTaTgU"},"source":["- Now we’ll implement **noun phrase** **chunking** to identify **named entities** using a **regular expression** consisting of rules that **indicate** how sentences should be **chunked**."]},{"cell_type":"markdown","metadata":{"id":"FQvjsrNQpQa2"},"source":["**<h4>Question 10:** How to perform NER (Named Entity Recognition) using NLTK library?</h4>\n","\n","<details>\n","\n","**<summary>Hint:</summary>**\n","\n","- Use **`nltk.ne_chunk()`** to perform NER..\n","\n","- Give `pos_tag` as parameter.\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"UCwBKtRiuhW9"},"source":["chunk = # Write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRJc3mvlpQa2"},"source":["def ner_fn(chunk):\n","    NE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n","\n","    return NE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2KvxnAHpQa3"},"source":["NE = ner_fn(chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IMRlyLFpQa3"},"source":["print(NE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCkguotlbJNG"},"source":["----\n","<a id=section6></a>\n","# **6. Conclusion**\n","----"]},{"cell_type":"markdown","metadata":{"id":"hhbh965UbNEG"},"source":["- **NER** gives us detailed knowledge about the **text** and the relationships between the **different entities**.\n","\n","- It is not **necessary** to apply all text **preprocessing techniques** on **NLP** tasks.\n","\n","- The pre-processing steps for a **problem** depend **mainly** on the **domain** and the **problem**.\n","\n","- Apply moderate pre-processing if you have a lot of **noisy data**, or if you have good quality text but a **scarcity** of data. When the **data** is **sparse**, heavy text pre-processing is needed. \n","\n","- NER can be **useful** in applications like **Classifying content for news providers**, **Summarizing Resumes**, **Optimizing Search Engine Algorithms**, **Simplifying Customer Support**."]}]}