{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_Using_Pre_trained_word_embedding.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xLlIpHoSACw9"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"xs-NhLo3AEbo"},"source":["<center><h1>Using Pre-trained Word Embedding</center>"]},{"cell_type":"markdown","metadata":{"id":"_smjm39JFkDq"},"source":["<center><table style=\"border: 2px solid black; border-collapse: collapse\">\n","\n","<tr>\n","<td style=\"border-right: 2px solid black; border-bottom: 2px solid black\"><img src=\"https://i.imgur.com/R8VLFs2.png\" height=\"200\"width=\"1050px\"/></td>\n","  \n","\n","<td style=\"border-right: 2px solid black; border-bottom: 2px solid black\"><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/GloVe.png\"/height=\"200\" width=\"1050px\"></td>\n","\n","<td style=\"border-right: 2px solid black; border-bottom: 2px solid black\"><img src=\"https://fasttext.cc/img/ogimage.png\"/ height=\"200\"width=\"1050px\"></td>\n","</tr>\n","\n","</table></center>"]},{"cell_type":"markdown","metadata":{"id":"a97bw2QpAEYp"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**What is Pre-trained Word Embedding?**](#section1)<br>\n","**2.** [**Popular used word embeddings**](#section2)<br>\n","   - **2.1** [**GloVe**](#section201)\n","   - **2.2** [**fastText**](#section202)\n","   - **2.3** [**Why use pre-trained word embbedding ?**](#section203)\n","\n","**3.** [**Instantiating an Embedding layer**](#section3)<br>\n","  - **3.1** [**Loading the IMDB data for use with an Embedding layer**](#section301)\n","  - **3.2** [**Note**](#section302)\n","  - **3.2.1** [**Mount your gdrive to your colab notebook**](#section30201)\n","  - **3.3** [**Preprocessing the Embedding file**](#section303)\n","    - **3.3.1** [**Importing Libraries**](#section30301)\n","    - **3.3.2** [**Define the path of embedding file**](#section30302)\n","    - **3.3.3** [**Read the embedding file**](#section30303)\n","      - **3.3.3.1** [**Embedding examples**](#section3030301)\n","     \n","**4.** [**Processing the raw IMDB data**](#section4)<br>\n","  - **4.1** [**Downloading the IMDB dataset**](#section401)\n","  - **4.2** [**Note**](#section402)\n","  - **4.3** [**Define the path for imdb data**](#section403)\n","  - **4.4** [**Read the training data only**](#section404)\n","  - **4.5** [**Tokenization**](#section406)\n","  - **4.6** [**Splits the data into a training set and a validation set**](#section407)\n","  - **4.7** [**Preparing the GloVe word-embeddings matrix**](#section408)\n","     \n","**5.** [**Preparing the model**](#section5)<br>\n","  - **5.1** [**Defining a model**](#section501)\n","  - **5.2** [**What about the embedding matrix which we create**](#section502)\n","    - **5.2.1** [**Loading pretrained word embeddings into the Embedding layer**](#section50201)\n","  - **5.3** [**Training and Evaluating the model**](#section503)\n","  - **5.4** [**Plot the model’s performance over time**](#section504)\n","  - **5.5** [**Important Note**](section505)\n","\n","**6.** [**Conclusion**](#section6)<br>"]},{"cell_type":"markdown","metadata":{"id":"YyDuQwIEBO4i"},"source":["---\n","<a name = Section1></a>\n","# **1. What is Pre-trained Word Embedding?**\n","---"]},{"cell_type":"markdown","metadata":{"id":"OyYNLS8vCIPt"},"source":["- Practitioners of deep learning for NLP typically **initialize** their models using **pre-trained word embeddings**, bringing in outside information, and **reducing** the number of **parameters** that a neural **network** needs to learn from scratch. \n"]},{"cell_type":"markdown","metadata":{"id":"5po4fUFLCx5x"},"source":["---\n","<a name = Section2></a>\n","# **2. Popular used word embeddings**\n","---"]},{"cell_type":"markdown","metadata":{"id":"X-0H253GnJ_C"},"source":["<a id=\"section201\"></a>\n","### **2.1 GloVe**"]},{"cell_type":"markdown","metadata":{"id":"1jxKmB0bClJq"},"source":["- [**GLOVE**](https://nlp.stanford.edu/projects/glove/) works similarly as **Word2Vec**.\n","\n","- While you can see above that Word2Vec is a **predictive** model that predicts **context** given word, GLOVE learns by **constructing** a **co-occurrence matrix** (words X context) that basically **count how frequently** a word appears in a context.\n","\n","- Since it's going to be a **gigantic** matrix, we **factorize** this matrix to achieve a **lower-dimension** representation. \n","\n","- There's a lot of details that goes in **GLOVE** but that's the **rough** idea.\n"]},{"cell_type":"markdown","metadata":{"id":"8-4Fo9GXmlmp"},"source":["<a id=\"section202\"></a>\n","### **2.2 FastText**"]},{"cell_type":"markdown","metadata":{"id":"Njr6Br4DCp54"},"source":["- [FastText](fasttext.cc/docs/en/english-vectors.html) is quite **different** from the above **2 embeddings**. \n","\n","- While **Word2Vec** and **GLOVE** treats each word as the smallest unit to train on, FastText uses **n-gram characters** as the **smallest** unit.\n","\n","- For example, the word **vector** ,**`apple`** could be broken down into **separate** word vectors **units** as **`ap,app,ple`**. \n","\n","- The biggest **benefit** of using FastText is that it generate **better word embeddings** for **rare words**, or even words not seen **during** training because the **n-gram** character **vectors** are shared with other words.\n","\n","- This is something that **Word2Vec** and **GLOVE** cannot achieve."]},{"cell_type":"markdown","metadata":{"id":"qYl4a6qVHITN"},"source":["<a id=\"section203\"></a>\n","### **2.3 Why use pre-trained word embbedding ?**"]},{"cell_type":"markdown","metadata":{"id":"xlzENIc9IE0f"},"source":["- Sometimes, you have so little **training** data available that you can't use your **data** along to learn an appropriate **task-specific** embedidng of your **vocabulary**. \n","\n","**What do you do them?**\n","\n","- Instead of **learning** word embeddings jointly with the **problem** you want to solve, you can load **embedding** vectors from **pre-computed** embedding space that you know is **highly structure** and **exhibits** useful properties \n","\n","- That capture **generic** aspect of **language** structure."]},{"cell_type":"markdown","metadata":{"id":"gxXyopASkaT2"},"source":["---\n","<a name = Section3></a>\n","# **3. Training NLP model with pre-trained word embedding**\n","---"]},{"cell_type":"markdown","metadata":{"id":"-T7BxgR3lYKq"},"source":["<a id=\"section301\"></a>\n","### **3.1 Download pretrained word embeddings**"]},{"cell_type":"markdown","metadata":{"id":"gOqdVoblkq1I"},"source":["-  [Download](https://nlp.stanford.edu/projects/glove/) the **precomputed** Glove **embeddings** trained on 2014 **English Wikipedia**.\n","\n","    - **Wikipedia** 2014 + **Gigaword** 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n","\n","-  [Download](https://fasttext.cc/docs/en/english-vectors.html) the precomputed fastText **embeddings** trained on 2017 **English Wikipedia**.\n","\n","- In this **notebook** we gonna use the **Glove embedding**.\n"]},{"cell_type":"markdown","metadata":{"id":"QocYZGZnncAJ"},"source":["<a id=\"section302\"></a>\n","### **3.2 Note**\n","\n","- Since the size of embedding is big, not possible to use and access the word embeddings to your local machine, so for that first download the __word embedding__ to your system and then __upload__ it to your __gdrive__."]},{"cell_type":"markdown","metadata":{"id":"ch5fLadBpDgn"},"source":["<a id=\"section30201\"></a>\n","#### **3.2.1 Mount your gdrive to your colab notebook**\n","\n","- While mounting the drive it require access form your gmail account."]},{"cell_type":"code","metadata":{"id":"g8AL6vwAnICW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620816779934,"user_tz":-330,"elapsed":50484,"user":{"displayName":"Ashish Lepcha","photoUrl":"","userId":"09506857476562598143"}},"outputId":"59f44218-b204-4fa2-f020-ab8898c37b4b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eZbT9tm_qLF_"},"source":["**Observation:**\n","\n","- Once drive is mounted find the __location__ where you stored your __Word Embedding__ file.Right click on it and __copy the path__ of the file."]},{"cell_type":"markdown","metadata":{"id":"s-3za4-mq1ZH"},"source":["<a id=\"section303\"></a>\n","\n","### **3.3 Preprocessing the Embedding file**"]},{"cell_type":"markdown","metadata":{"id":"oEgBUAL4rItO"},"source":["- Since the __GloVe embedding__ available with different __dimensional vectors__ i.e **50-D,100-D,200-D,300-D**.\n","\n","\n","- Here we are using **50D** because less **dimension** required less model **training** time. "]},{"cell_type":"markdown","metadata":{"id":"ArX3vm9Tsr9b"},"source":["<a id=\"section30301\"></a>\n","#### **3.3.1 Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"ItkIWd-QuyJn"},"source":["# Import tensorflow 2.x\n","# This code block will only work in Google Colab.\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgUk7irRsqS-"},"source":["import numpy as np         # For performing mathematical operations \n","import pandas as pd        # For data analysis \n","import os\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Flatten, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTTJpKIVtgr8"},"source":["<a id=\"section30302\"></a>\n","#### **3.3.2 Define the path of embedding file**\n"]},{"cell_type":"markdown","metadata":{"id":"YX3EpWV0P6nm"},"source":["- **Download** the **`glove.6B.50d.txt`** from [here](https://www.kaggle.com/watts2/glove6b50dtxt?select=glove.6B.50d.txt)"]},{"cell_type":"markdown","metadata":{"id":"8mJUh39Soezy"},"source":["**Important Note:**\n","\n","Your path could be **different** so use your's one.\n"]},{"cell_type":"code","metadata":{"id":"ydE7uuF8tu_F"},"source":["# Define the path of embeddings\n","glove_dir = \"/content/drive/My Drive/Word embeddings/glove.6B.50d.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnu3CEsctyHr"},"source":["<a id=\"section30303\"></a>\n","\n","#### **3.3.3 Read the embedding file**"]},{"cell_type":"code","metadata":{"id":"CbS8aCCfAnZm"},"source":["embeddings_index = {}    # Dictionary to store embedding vectors with it's value.\n","f=open(glove_dir)\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9CksyDXuPdr"},"source":["<a id=\"section303030301\"></a>\n","#### **3.3.3.1 Embedding examples**\n","\n","- Let's see few embedding examples for different words.\n","\n","- In **embedding** all words are stored in **lower case** so make sure you word need to be in lower case before **testing**.\n"]},{"cell_type":"code","metadata":{"id":"muZR1PZasSc3"},"source":["print(\"The 50 dimensional embedding vector for word hello is {}\".format(embeddings_index['hello']))\n","print('-------------------'*10)\n","\n","print(\"The 50 dimensional embedding vector for word data is {}\".format(embeddings_index['data']))\n","\n","print('-------------------'*10)\n","\n","print(\"The 50 dimensional embedding vector for word science is {}\".format(embeddings_index['science']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdjhwF20xi2v"},"source":["---\n","<a name = Section4></a>\n","# **4. Processing the raw IMDB data**\n","---"]},{"cell_type":"markdown","metadata":{"id":"FgDeEPdO2C0m"},"source":["### **4.1 Downloading the IMDB dataset**\n","\n","<br>\n","\n","- [Download](http://mng.bz/0tIo) the imdb dataset. "]},{"cell_type":"markdown","metadata":{"id":"ctQwRzD5h281"},"source":["<a id=\"section402\"></a>\n","### **4.2 Note**"]},{"cell_type":"markdown","metadata":{"id":"7NNnSnH82ocr"},"source":["- Like previously, we will first upload the **`movie_review`** data to gdrive and then read it. If the upoloaded file is zipped one then first upzip it and then read the file using it's path in the driver.\n","\n","-  Here we uploaded the file as zipped one to the drive, it saves a lot drive space and time."]},{"cell_type":"code","metadata":{"id":"y-lPmD-FuepY"},"source":["# unzip the file\n","\n","# It's time consuming process so grab some coffee and wait and watch.\n","\n","! unzip \"/content/drive/My Drive/Dataset/Movie_review.zip\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ko5xpqKk9H4-"},"source":["**Observation:**\n","\n","- After **unziping** got two new file, we need only file with name **aclImDB**."]},{"cell_type":"markdown","metadata":{"id":"gq_fh1C9-DXm"},"source":["<a id=\"section403\"></a>\n","### **4.3 Define the path for imdb data**"]},{"cell_type":"code","metadata":{"id":"fz1xUWsiwY6P"},"source":["imdb_dir=\"/content/aclImdb\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0ZXSP3--SUt"},"source":["<a id=\"section404\"></a>\n","### **4.4 Reading the training file.**"]},{"cell_type":"markdown","metadata":{"id":"6OrtoZ-Rz_f6"},"source":["**Movie_review** data have train and test data here we **read** only train data and according to that create **seperate** labels for each **review**. i.e Whether it is __-ve__ or __+ve__.\n"]},{"cell_type":"code","metadata":{"id":"8SSglEAUyeqA"},"source":["train_dir = os.path.join(imdb_dir, 'train')\n","labels = []\n","texts = []\n","for label_type in ['neg', 'pos']:\n","  dir_name = os.path.join(train_dir, label_type)\n","  for fname in os.listdir(dir_name):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname))\n","      texts.append(f.read())\n","      f.close()\n","      # assigning a label to the review\n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyVn2_aG2E2W"},"source":["<a id=\"section406\"></a>\n","### **4.5 Tokenization** "]},{"cell_type":"markdown","metadata":{"id":"y6EhFXft1jkS"},"source":["- From keras import **tokenizer** and **pad_sequences** so we can tokenize the review and perform padding on each one to have **same length**.\n"]},{"cell_type":"code","metadata":{"id":"wrcB93sU_bAU"},"source":["maxlen = 50                  # Cuts off reviews after 50 words\n","training_samples = 200       # Trains on 200 samples\n","\n","validation_samples = 10000   # Validates on 10,000 samples\n","\n","max_words = 10000            # Considers only the top 10,000 words in the dataset\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","data = pad_sequences(sequences, maxlen=maxlen)\n","labels = np.asarray(labels)\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRIKwIE32UjK"},"source":["<a id=\"section407\"></a>\n","### **4.6 Splits the data into a training set and a validation set**\n"]},{"cell_type":"markdown","metadata":{"id":"UgTI0q1XohJU"},"source":["- Before **split** the data first **shuffle** the data, because we started with data in which samples are **ordered** (**all negative** first, then **all positive**) "]},{"cell_type":"code","metadata":{"id":"iLsayRxRAAmC"},"source":["indices = np.arange(data.shape[0])\n","\n","np.random.shuffle(indices)   # Create a random shuffle\n","\n","data = data[indices]         # Based on the shuffle create a new data \n","labels = labels[indices]\n","\n","## Train and validation \n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbVSSGQs4B5C"},"source":["<a id=\"section407\"></a>\n","###  **4.7 Preparing the GloVe word-embeddings matrix**"]},{"cell_type":"code","metadata":{"id":"xTpuC0FOlap-"},"source":["embedding_dim = 50       \n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","  if i < max_words:\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","      embedding_matrix[i] = embedding_vector   # Words not found in the embedding index will be all zeros. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKAWtWI646lr"},"source":["---\n","<a name = Section5></a>\n","# **5. Preparing the model**\n","---"]},{"cell_type":"markdown","metadata":{"id":"7sft0q80-WE7"},"source":["<a id=\"section502\"></a>\n","### **5.1 Defining a model**"]},{"cell_type":"markdown","metadata":{"id":"xkWx1l1lKOFk"},"source":["- We are using **`Sequential`** function from **`tensorflow.keras.models`** to build our model.\n","\n","- We are using a **embedding layer** using our **pre-trained** word embeddings.\n","\n","- We are using **flatten** and **dense layers** to build our model."]},{"cell_type":"code","metadata":{"id":"E6FB1m4aANde"},"source":["model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqqq1BZe7LJI"},"source":["<a id=\"section502\"></a>\n","### **5.2 What about the embedding matrix which we create**"]},{"cell_type":"markdown","metadata":{"id":"ynISos7v5xY6"},"source":["<center>\n","<td><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/Question.png\"/ width=\"150px\"></td>\n","\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"-ldr7Fz-77aU"},"source":["<a id=\"section50201\"></a>\n","#### **5.2.1 Loading pretrained word embeddings into the Embedding layer**\n"]},{"cell_type":"code","metadata":{"id":"2lQ4NrnM6u_p"},"source":["# set the weights in accordance with the pre-trained model.\n","model.layers[0].set_weights([embedding_matrix])\n","\n","# As we don't need to train for weights so freeze the Embedding layer (set its trainable attribute to False)\n","model.layers[0].trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDY4fVp18b0J"},"source":["<a id=\"section503\"></a>\n","### **5.3 Training and Evaluating the model**"]},{"cell_type":"code","metadata":{"id":"Iyq9KdwEltwt"},"source":["model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n","history = model.fit(x_train, y_train,epochs=20,batch_size=32,validation_data=(x_val, y_val))\n","model.save_weights('pre_trained_glove_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5i7pPMg9Gcb"},"source":["**Observation:**\n","\n","- Don't need to **bother** about what **output** we got after **training** because here we train for only **200 review**."]},{"cell_type":"markdown","metadata":{"id":"aX1LrOt--QMc"},"source":["<a id=\"section504\"></a>\n","### **5.4 Plot the model’s performance over time**"]},{"cell_type":"code","metadata":{"id":"mqZ4B3nZl4US"},"source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_PcHZqJd-GY"},"source":[" <a id=\"section505\"></a>\n","### **5.5 Important Note**"]},{"cell_type":"markdown","metadata":{"id":"nTfGduNe_HZG"},"source":["- The model quickly starts **overfitting**, which is **unsurprising** given the small number of **training** samples.\n","\n","- Because you have so few training samples, **performance** is heavily **dependent** on exactly which **200** samples you choose and you’re choosing them at **random**. \n","\n","- If this works poorly for you, try choosing a different **random** set of **200 samples**, for the sake of the **exercise** or try it by **increase** the number of **samples** like try to train with **5000** samples."]},{"cell_type":"markdown","metadata":{"id":"WT0MhKFmAXA9"},"source":["---\n","<a name = Section6></a>\n","# **6. Conclusion**\n","---"]},{"cell_type":"markdown","metadata":{"id":"MhYbPnP_AwZ8"},"source":["- After this sheet you are **aware** about how to use it but when to use it. \n","\n","- So for the use case of **word-embeddings** when you have **limited data** set that **time** use word embeddings.\n","\n","__When lots of data is available__:\n","\n","- Train the model with **loading** the pre-trained word *embeddings* and **without freezing** the embedding layer. \n","\n","- In that case, you’ll learn a tasks pecific embedding of the **input tokens**, which is generally more **powerful** than **pre-trained** word embeddings. \n","\n"]}]}