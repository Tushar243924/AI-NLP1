{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_Embedding_Layer_in_Neural_Network.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"DJ0gKr7gydy4"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"1ekKNBj8yklB"},"source":["<center><h1>Embedding Layer in Neural Network</center> "]},{"cell_type":"markdown","metadata":{"id":"smRX8ZV8yoWo"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**What is word embedding?**](#section1)<br>\n","  - **1.1** [**Why You Need to Start Using Embedding Layers**](#section101)\n","  - **1.2** [**How to obtaion word embedding**](#section102)\n","\n","**2.** [**Learning Word Embeddings for IMDB Movie Review Data**](#section2)<br>\n","  - **2.1** [**Imdb Movie Review data**](#section201)\n","    - **2.1.1** [**Overview**](#section20101)\n","    - **2.1.2** [**Dataset**](#section20102)\n","\n","**3.** [**Instantiating an Embedding layer**](#section3)<br>\n","  - **3.1** [**Loading the IMDB data for use with an Embedding layer**](#section301)\n","  - **3.2** [**Let's Train the model**](#section302)\n","     \n","**4.** [**Conclusion**](#section4)<br>"]},{"cell_type":"markdown","metadata":{"id":"i4ZHZ-TNyrKW"},"source":["---\n","<a name = Section1></a>\n","# **1. What is word embedding?**\n","---"]},{"cell_type":"markdown","metadata":{"id":"pOj27OD5yu3f"},"source":["- A **word embedding** is a learned representation for text where words that have the same meaning have a similar **representation**.\n","\n","- It provide a **dense representation** of words and their relative meanings.\n","\n","- Because of word embedding words of similar **context** are close to **each** **other**.\n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/Word-Vectors.png\" width=\"840\" height=\"300\" /></center>\n"]},{"cell_type":"markdown","metadata":{"id":"2ECg_UWRyyXi"},"source":["<a name = Section11></a>\n","### **1.1 Why You Need to Start Using Embedding Layers?**"]},{"cell_type":"markdown","metadata":{"id":"WqqAeMiby1h-"},"source":["- **One-hot encoded** vectors are **high-dimensional** and **sparse**.\n","\n","- Let’s **assume** that we are doing Natural Language Processing (NLP) and have a dictionary of **2000** words. \n","\n","- This means that, when using **one-hot encoding**, each word will be represented by a **vector** containing **2000 integers**. And **1999** of these integers are **zeros**.\n","\n","- In a big dataset this **approach** is not **computationally** efficient.\n","\n","- The vectors of each **embedding** get updated while **training** the neural network. \n","\n","- If you have seen the **image** at the top of this post you can see how **similarities** between words can be found in a **multi-dimensional** space. \n","\n","- This allows us to **visualize** relationships between words, but also between everything that can be **turned** into a **vector** through an **embedding layer**."]},{"cell_type":"markdown","metadata":{"id":"m45uhOJ2y_Uk"},"source":["<a name = Section12></a>\n","### **1.2 How to obtain word embedding?**"]},{"cell_type":"markdown","metadata":{"id":"AjmwKpAwzFoa"},"source":["- Learn word embeddings **jointly** with the main task you care about(such as document **classification** or **sentiment prediction**). \n","\n","- In this setup, you start with **random** word vectors and then **learn** word vector in the same way you learn the **weight** of a **neural network**.\n","\n","- Load into your model **word embeddings** that were **pre-computed** using different machine-learning task tha the one you're trying to solve. These are called **pretrained** word embeddings\n","\n","- Two **popular** word embeddings are\n","    - **GloVe** \n","    \n","    - **fastText**"]},{"cell_type":"markdown","metadata":{"id":"kFa7tgMszIzP"},"source":["---\n","<a name = Section2></a>\n","# **2. Learning Word Embeddings for IMDB Movie Review Data**\n","---"]},{"cell_type":"markdown","metadata":{"id":"vrmBCVq9_P6B"},"source":["<a id=\"section201\"></a>\n","### **2.1 Imdb Movie Review data**"]},{"cell_type":"markdown","metadata":{"id":"d5z32Du5zMGH"},"source":["<a id=\"section20101\"></a>\n","#### **2.1.1 Overview**\n","\n","- This dataset contains movie reviews along with their associated binary **sentiment polarity labels**. \n","\n","- It is intended to serve as a benchmark for **sentiment classification**. \n","\n","- This **document** outlines how the dataset was gathered, and how to use the files **provided**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qyrUA3Kd_Yzy"},"source":["<a id=\"section20102\"></a>\n","#### **2.1.2 Dataset**\n","\n","- The core dataset contains **50,000** reviews split evenly into **25k** train and 25k test sets. \n","\n","- The overall **distribution** of labels is balanced (**25k pos** and **25k neg**).\n","\n","- In the entire **collection**, no more than **30** reviews are allowed for any given movie because **reviews** for the same movie tend to have **correlated ratings**.\n","\n","- Further, the train and test sets contain a **disjoint** set of **movies**, so no significant **performance** is obtained by **memorizing** movie-unique terms and their **associated** with **observed** labels.\n","\n","- In the labeled **train/test** sets, a **negative** review has a **score <= 4** out of 10, and a **positive** review has a **score >= 7** out of 10. \n","\n","- Thus reviews with more **neutral ratings** are not included in the **train/test** sets.\n","\n","- In the **unsupervised** set, reviews of any **rating** are included and there are an even number of **reviews** > 5 and <= 5."]},{"cell_type":"markdown","metadata":{"id":"Zr6WqhgVzPIQ"},"source":["---\n","<a name = Section3></a>\n","# **3. Instantiating an Embedding layer**\n","---"]},{"cell_type":"code","metadata":{"id":"ItkIWd-QuyJn","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b56cdedf-e2a7-42a4-f80b-bbcfd4143a33"},"source":["# Import tensorflow 2.x\n","# This code block will only work in Google Colab.\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1DyOWZ_-vVG1"},"source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras import preprocessing\n","embedding_layer = Embedding(1000, 64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u82mwFdksk2A"},"source":["- Here we first need to install the **required** version of numpy for keras.\n","\n","- Because the current numpy version is not **compatible** with keras.\n","\n","- Until then, try **downgrading** your numpy version to **1.16.2.** It seems to solve the **problem**.\n","\n","- If your **current** version of numpy works **properly** with the upcoming code, then there's no need to **downgrade** it."]},{"cell_type":"code","metadata":{"id":"Xny7wHYM4uHy"},"source":["# !pip install numpy==1.16.1\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNUAxux50k5C"},"source":["<a id=\"section301\"></a>\n","### **3.1 Loading the IMDB data for use with an Embedding layer**"]},{"cell_type":"markdown","metadata":{"id":"0VgsV2nVvYKB"},"source":[" - We will consider **10000** most common words."]},{"cell_type":"code","metadata":{"id":"Owtx-7T9vtAu"},"source":["max_features = 10000  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uw75On7Xvtqv"},"source":[" - We will consider only first **20** words for each movie review.\n"]},{"cell_type":"code","metadata":{"id":"Gk-f5Y8bvzJA"},"source":["maxlen = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tOkGhKicwJFs"},"source":["- Load the data as **lists of integers**, which is already done in keras "]},{"cell_type":"code","metadata":{"id":"-p4SZTHgwWV_"},"source":["(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tYb1w8YwaQq"},"source":["- Transform the lists of integers into a **2D integer** tensor of shape (samples, maxlen)"]},{"cell_type":"code","metadata":{"id":"cCEQ5Au_zR36"},"source":["x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)  \n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3jjYuMQ3CwD"},"source":["- Print the **shape** for both **train** and **test** data"]},{"cell_type":"code","metadata":{"id":"wg0V4M980rqX","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"39a48b75-4243-411a-9806-7e41fc8c38a7"},"source":["print(x_train.shape)\n","\n","print(x_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(25000, 20)\n","(25000, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6zuDGzNX6He_"},"source":["<a id=\"section302\"></a>\n","### **3.2 Let's Train the model**"]},{"cell_type":"markdown","metadata":{"id":"s77_gxBkww4d"},"source":["- Import **keras sequential api** to create models **layer-by-layer**."]},{"cell_type":"code","metadata":{"id":"ptahnUPowu55"},"source":["model = Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdlILM8Xxjey"},"source":["- Specifies the **maximum** input length to the **Embedding layer** so you can later flatten the **embedded** inputs.\n","\n","- After the **Embedding layer**, the activations have **shape** (samples, maxlen, 8)."]},{"cell_type":"code","metadata":{"id":"gN9yY19yxe8L"},"source":["model.add(Embedding(10000, 8, input_length=maxlen))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lm0aXsxS0Qz1"},"source":["-  **Flatten** layer in Keras "]},{"cell_type":"code","metadata":{"id":"018hjSl70TUD"},"source":["model.add(Flatten())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmtzaGrx0VkI"},"source":["- Adds the **classifier** on top"]},{"cell_type":"code","metadata":{"id":"28BIbo2w0hGj"},"source":["model.add(Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7x_gf8J1Fcg"},"source":["#### What does compile do?\n","\n","- **Compile** defines the **loss function**, the **optimizer** and the metrics. "]},{"cell_type":"code","metadata":{"id":"alEH1XBh1TJ0"},"source":["model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"emmpu5L21w0X"},"source":["model.summary() #Print the summary representation of your model."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmIab8GC15On"},"source":["- Training the model on the **custom movie data**."]},{"cell_type":"code","metadata":{"id":"JrhToVYG5TYu","colab":{"base_uri":"https://localhost:8080/","height":836},"outputId":"c31f7764-645a-4641-c7a3-5a8b8fcf6f15"},"source":["history = model.fit(x_train, y_train,epochs=10,batch_size=32,validation_split=0.2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W0827 07:15:19.396104 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0827 07:15:19.415315 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0827 07:15:19.463059 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0827 07:15:19.483021 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0827 07:15:19.489022 139633655900032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 160)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["W0827 07:15:19.725079 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n","20000/20000 [==============================] - 2s 106us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6812\n","Epoch 2/10\n","20000/20000 [==============================] - 1s 63us/step - loss: 0.5658 - acc: 0.7425 - val_loss: 0.5467 - val_acc: 0.7204\n","Epoch 3/10\n","20000/20000 [==============================] - 1s 60us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n","Epoch 4/10\n","20000/20000 [==============================] - 1s 59us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n","Epoch 5/10\n","20000/20000 [==============================] - 1s 61us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n","Epoch 6/10\n","20000/20000 [==============================] - 1s 59us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n","Epoch 7/10\n","20000/20000 [==============================] - 1s 58us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n","Epoch 8/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n","Epoch 9/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.3022 - acc: 0.8767 - val_loss: 0.5213 - val_acc: 0.7492\n","Epoch 10/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GGQufrV7-E7m"},"source":["---\n","<a name = Section4></a>\n","# **4. Conclusion**\n","---"]},{"cell_type":"markdown","metadata":{"id":"BDokqHFl824u"},"source":["- You get to a validation accuracy of **~76%**, which is pretty good considering that you’re only looking at the first **20** words in every review. \n","\n","- But note that merely **flattening** the **embedded sequences** and training a **single Dense layer** on top leads to a model that **treats** each word in the input sequence **separately**, without considering **inter-word** relationships and **sentence structure** (for example, this model would likely treat both **`this movie is a bomb`** and **`this movie is the bomb`** as **being negative reviews**.\n",".\n","- It’s much better to add **recurrent layers** or **1D convolutional layers** on top of the **embedded** sequences to **learn** features that take into account each **sequence** as a whole."]}]}