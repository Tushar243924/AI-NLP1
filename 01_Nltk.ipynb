{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Nltk.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"hpT02vsTow48"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"oRSxxSSV7PvD"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**Introduction**](#section1)<br>\n","**2.** [**Text Preprocessing**](#section2)<br>\n","  - **2.1** [**Convert Text to Lower Text**](#section201)\n","  - **2.2** [**Word Tokenize**](#section202)\n","  - **2.3** [**Sent Tokenize**](#section203)\n","  - **2.4** [**Stop words removal**](#section204)\n","  - **2.5** [**Lemma**](#section205)\n","  - **2.6** [**Stem**](#section206)\n","  - **2.7** [**Get word frequency**](#section207)\n","  - **2.8** [**Pos Tags**](#section208)\n","  - **2.9** [**NER(Named Entity Recognition)**](#section209)\n","\n","**3.** [**Bag of Words Approach**](#section3)<br>\n","**4.** [**Applications**](#section4)<br>"]},{"cell_type":"markdown","metadata":{"id":"LLdbpjQY1glw"},"source":["---\n","<a name = Section1></a>\n","# **1. Introduction**\n","---"]},{"cell_type":"markdown","metadata":{"id":"L0Y-s377c2tS"},"source":["- Field of Computer Science and Linguistics: **interaction** between **computers** and **human** languages.\n","\n","<br>  \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/human-comp.jpg\"width=\"440\" height=\"300\"/></center>\n","\n","<br>  \n","- The main objective is to **enrich** computer algorithms with the **capacity** of handling **natural** language."]},{"cell_type":"markdown","metadata":{"id":"UsOQbFRrwXLN"},"source":["<center><img src = \" https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/comphuman.JPG\"width=\"640\" height=\"250\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"ejglSvBAjrFE"},"source":["- Text often contain **slang, sarcasm, inconsistent syntax, grammatical errors, double entendres**, etc. \n","\n","- Those create a **problem** for the machine algorithms as all **text mining** techniques require correctly **structured** text sentences or documents. \n","\n","- So, if you still want to **implement** an algorithm, someone needs to correct all writing **errors** and **standardize** word forms to make them more **comparable**. \n","\n","- The best way to do that is with **natural language processing** tools."]},{"cell_type":"markdown","metadata":{"id":"h7SFAtyOkC-F"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nlp222.JPG\" width=\"440\" height=\"300\"></center>"]},{"cell_type":"markdown","metadata":{"id":"iGxKw7Sr0eaB"},"source":["#### What is the need for the usage of NLP libraries?"]},{"cell_type":"markdown","metadata":{"id":"hN-TgSDm0cEn"},"source":["- These **Libraries** helps us to **extract meaning** from the **text**.\n","\n","- It includes the **wide** range of tasks such as **document classification, topic modelling, part-of-speech (POS) tagging, and sentiment analysis** etc."]},{"cell_type":"markdown","metadata":{"id":"AsKhraF8O5ik"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nlplibraries.JPG\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"QI8G1Lqz0w5O"},"source":["#### Let’s have a look at the following libraries:-\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ltk1.JPG\" width=\"140\" height=\"80\"/></center>\n","\n","#### Natural Language Toolkit(NLTK)\n","\n","- It is foremost platform for **constructing python programs** when we deal with linguistic data. \n","\n","-  It is a suite of **libraries** and **programs** for symbolic and **statistical** natural language processing (NLP) for **English written** in the Python programming language.\n","\n","- It is **open source, community-driven project.**\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/scikit.JPG\"width=\"160\" height=\"140\"/></center>\n","\n","#### Scikit-learn\n","\n","- It is not just used for **NLP** but it’s applied widely in **Machine learning**.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/textblob.JPG\" width=\"240\" height=\"200\"/></center>\n","\n","#### TextBlob\n","\n","- It provides with **NLP tools API** which can be **easily handled** by the users.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/spacy.JPG\"width=\"140\" height=\"80\"/></center>\n","\n","#### spaCy\n","\n","- Using spacy, we can implement concepts of NLP by using **Python and Cython**.\n","\n","- It has some **excellent capabilities** for **named entity recognition**.\n","\n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/polyglot2.JPG\"width=\"240\" height=\"160\"/></center>\n","\n","#### Polyglot\n","\n","- It is the yet **another python package** for NLP. \n","\n","- It is **not very popular** but also can be used for a **wide range of the NLP tasks**.\n","\n","- It is usually used for **projects involving a language** spaCy doesn’t support.\n","\n","\n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/gensim.JPG\" width=\"200\" height=\"100\"/></center>\n","\n","#### Gensim\n","\n","- It is specifically used for **Topic Modelling** while we deal with text documents.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stanford.JPG\"width=\"140\" height=\"150\"/></center>\n","\n","#### Stanford Core NLP\n","\n","- One of the most famous university i.e. **Stanford University** has contributed a lot towards the **development of NLP.**\n","\n","- There are numerous **NLP packages and services** which are built by Stanford NLP Group for the same purpose."]},{"cell_type":"markdown","metadata":{"id":"ZZpysumToh56"},"source":["#### Lets get started with Natural Language Processing \n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/1_tzWP1K-eXiehDBBUxUN11w.gif\"width=\"640\" height=\"300\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"KTVfJrTI4GFP"},"source":["#### The data obtained from different sources is full of noise i.e. errors which will affect the precision of the result.\n","\n","- So, we need to perform **cleaning** and **standardization** of the text. \n","\n","- This will make the **text noise free** and **ready** for further analysis."]},{"cell_type":"markdown","metadata":{"id":"QeuJSWzWzv9M"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/strudata.JPG\" width=\"420\" height=\"200\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"TGiW2VpJGfSq"},"source":["#### Forms of NLP data:"]},{"cell_type":"markdown","metadata":{"id":"oUk7dd0DkDOI"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ictss.JPG\"width=\"840\" height=\"480\"/></center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pQr8a0pb1nPy"},"source":["---\n","<a name = Section2></a>\n","# **2. Text Preprocessing**\n","---"]},{"cell_type":"markdown","metadata":{"id":"oV6QrrcCdvUr"},"source":["**The basic tasks in NLP are**:\n","\n","**1. Lowercasing**\n","\n","**2. Word tokenize**\n","\n","**3. Sentence tokenize**\n","\n","**4. Stop words removal**\n","\n","**5. Lemmatization(Lemma)**\n","\n","**6. Stemming(Stem)**\n","\n","**7. Get word frequency**\n","\n","**8. Part of Speech tags(POS)**\n","\n","**9. Named Entity Recognition(NER)**"]},{"cell_type":"markdown","metadata":{"id":"rd9i1W7Qnp3Z"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/caps.JPG\"width=\"540\" height=\"300\"></center>"]},{"cell_type":"markdown","metadata":{"id":"3FSw4G9QeFTC"},"source":["#### Pre-requirements:\n","\n","- Install **Python**\n","\n","- Install **NLTK** and its corpus"]},{"cell_type":"markdown","metadata":{"id":"0k8nyKzEu2dg"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/cp1.JPG\" width=\"340\" height=\"300\"/></center>\n","\n","#### Corpus \n","\n","- It is a **large collection of text.**\n","\n","- The **Plural** of corpus is **Corpora.**\n","\n","- It is written of spoken material on which **linguistic analysis** is used and is based upon.\n","\n","- Its data is **stored** in **library installed** .\n","\n","- **For Example** - Nltk library has following corpus : \n","\n","  - Brown corpus\n","\n","  - Gutenberg corpus\n","\n","  - Shakespeare corpus\n","\n","  - Stop words\n","\n","  - Treebank corpus etc\n"]},{"cell_type":"markdown","metadata":{"id":"9ZmnMg8PMzS7"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/corpus2.JPG\"width=\"460\" height=\"390\"/></center>\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/2fw2xi.gif\"width=\"440\" height=\"280\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"uYfbZNpPdjo_"},"source":["#### Why Corpus?\n","\n","- It provides **descriptive insights** relevant to how people **use language**.\n","\n","- It acts as tool that enables to **analyse** both how people use **different language forms** at various levels of formality.\n","\n","- It helps analyse how language fulfils **multiple speech functions** across contexts."]},{"cell_type":"markdown","metadata":{"id":"ixgt0spfJbGD"},"source":["#### Lets install Brown corpus"]},{"cell_type":"code","metadata":{"id":"p3ytsQS9Sou9","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"f3f9c6af-9d65-4809-9f9c-2a4d64df6e85"},"source":["import nltk\n","nltk.download('brown')\n","import warnings\n","warnings.filterwarnings(action = 'ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GhG2q9beJpFX","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"b5fbadbe-221f-4c21-80e9-aadccf6e703c"},"source":["from nltk.corpus import brown\n","brown.categories()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['adventure',\n"," 'belles_lettres',\n"," 'editorial',\n"," 'fiction',\n"," 'government',\n"," 'hobbies',\n"," 'humor',\n"," 'learned',\n"," 'lore',\n"," 'mystery',\n"," 'news',\n"," 'religion',\n"," 'reviews',\n"," 'romance',\n"," 'science_fiction']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"eVWd2kRZoLZq"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/lady.jpg\" width=\"440\" height=\"300\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"jU_Zq2WHoDv-"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/oni.JPG\" width=\"390\" height=\"280\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"gp6Z2soIePfN"},"source":["#### Import nltk in order to use its functions."]},{"cell_type":"code","metadata":{"id":"bbHEtJkFefxb"},"source":["import nltk\n","Br = nltk.corpus.brown.words() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IAeLVjTTo9H"},"source":["#### Lets add words to the corpus "]},{"cell_type":"code","metadata":{"id":"sfx9YkoPSk-1"},"source":["l = list(Br)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djwxl2fSTzih"},"source":["#### Let the word be 'talking'"]},{"cell_type":"code","metadata":{"id":"m7D_aAP4SkqH"},"source":["l.append('talking')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWSzdBcBzJUT"},"source":["#### NLTK: Some Example Modules\n","\n","- **nltk.token**: processing **individual** elements of text, such as words or sentences.\n","\n","-  **nltk.probability**: modeling **frequency distributions** and probabilistic systems.\n","\n","-  **nltk.tagger**: tagging tokens with **supplemental** information, such as parts of speech or wordnet sense tags.\n","\n","- **nltk.parser**: high-level interface for **parsing** texts.\n","\n","-  **nltk.chartparser**: a **chart-based** implementation of the parser interface.\n","\n","- **nltk.chunkparser**: a **regular-expression** based surface parser."]},{"cell_type":"markdown","metadata":{"id":"6XfQFdJBeoK1"},"source":["<a id=section201></a>\n","### **2.1 Coverting text to lower text as it is case sensitive**"]},{"cell_type":"markdown","metadata":{"id":"PF8uuumNVvuB"},"source":["<center><img src= \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/wrong.JPG\" width=\"380\" height=\"300\"/></center>"]},{"cell_type":"code","metadata":{"id":"kZEkdTpneqFI","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8d15d452-91e9-4f3a-fdf4-d539a50fe045"},"source":["text =  \"Full form of NLTK is Natural Language Toolkit\"\n","lower_text = text.lower()\n","print (lower_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["full form of nltk is natural language toolkit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SBW6so118Jfs"},"source":["<a id=section202></a>\n","### **2.2 Word tokenize**"]},{"cell_type":"markdown","metadata":{"id":"wymdpTnnWKFE"},"source":["- The task of converting a text from a **single string** to a **list of tokens** is known as tokenization.\n","\n","- The tokens may be **words** or **number** or **punctuation mark**. \n","\n","   - Word tokenisation.\n","\n","   - Sentence tokenisation\n","   \n"," <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/tokenis.JPG\"width=\"340\" height=\"280\"/></center>\n","\n"," \n","#### Why tokenisation?\n","\n"," - Computers cannot understand **large chunks** of data.\n","\n"," - As a part of **preprocessing**, the text is broken into pieces called **Tokens**.\n","\n"," - This is important because the **meaning of text** generally depends on the **relations of words** in that text.\n","\n"," - These Tokens are **fed as an input** for other types of **analysis or tasks**."]},{"cell_type":"markdown","metadata":{"id":"25OFKHH-ewVA"},"source":["- Tokenize sentences to get the **tokens** of the **text** i.e **breaking the sentences** into words."]},{"cell_type":"code","metadata":{"id":"n4NqLmp_e5zN"},"source":["import nltk\n","from nltk import sent_tokenize, word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXgG68U78X9l"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/891.JPG\"width=\"580\" height=\"180\"/></center>\n"]},{"cell_type":"code","metadata":{"id":"29tM5HLefGQf","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"49a18927-0f33-4113-96a2-ab833895451f"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Y4txZ2iGe98C","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3dbb59f9-5b85-4988-e4af-58a5c1f2bb81"},"source":["text = \"Full form of NLTK is Natural Language Toolkit\"\n","word_tokens = nltk.word_tokenize(text)\n","print (word_tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KCBlef_L8cFB"},"source":["<a id=section203></a>\n","### **2.3 Sent tokenize**"]},{"cell_type":"markdown","metadata":{"id":"urv8ij73fM1U"},"source":["- Tokenize sentences if there are **more than 1 sentence** i.e **breaking** the **sentences** to **list of sentence**."]},{"cell_type":"code","metadata":{"id":"6BeErMnBfSJY","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d2222e13-7836-4835-99ae-9c344ee63b1c"},"source":["text = \"Full form of NLTK is Natural Language Toolkit. It is a very powerful Package.\"\n","sent_token = nltk.sent_tokenize(text)\n","print (sent_token)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Full form of NLTK is Natural Language Toolkit.', 'It is a very powerful Package.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GE_kzxjM8g6R"},"source":["<a id=section24></a>\n","### **2.4 Stop words removal**"]},{"cell_type":"markdown","metadata":{"id":"zM73kGIZfZNT"},"source":["- Remove **irrelevant words** using **nltk stop words** like is,the,a etc from the sentences as they **don’t carry any information**."]},{"cell_type":"markdown","metadata":{"id":"ybYUySG4kua8"},"source":["<center> <img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stopwords.JPG\"width=\"440\" height=\"240\"/> </center>"]},{"cell_type":"code","metadata":{"id":"3HdcCYnLfckx"},"source":["import nltk\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i3xxBAWSfr5g","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"4c2aeb3d-c7e8-42f5-d1b0-f78127415768"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"pK-AvxPooerW"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stop%20words.JPG\" width=\"420\" height=\"300\"></center>"]},{"cell_type":"code","metadata":{"id":"rAGjxjWfffVe","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a3027e7d-c2b0-4276-94d6-a198bf780774"},"source":["stopword = stopwords.words(\"english\")\n","text =  \"Full form of NLTK is Natural Language Toolkit\"\n","word_tokens = nltk.word_tokenize(text)\n","removing_stopwords = [word for word in word_tokens if word not in stopword]\n","print (removing_stopwords)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FVsIW4pm8nnB"},"source":["<a id=section205></a>\n","### **2.5 Lemmatization**"]},{"cell_type":"markdown","metadata":{"id":"gINKqoZOfzTR"},"source":["- **Lemmatize the text** so as to get its **root form** \n","\n","- For Example: **functions,funtionality as function**"]},{"cell_type":"markdown","metadata":{"id":"Nz5wmF1IoFou"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/89.JPG\" height = \"200\"/></center>"]},{"cell_type":"code","metadata":{"id":"GXqH7plUf09_"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","#is based on The Porter Stemming Algorithm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGXf7ZovgRZ-","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"cf834307-dd14-47af-f15a-5e81233d8175"},"source":["nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"N1DEos4pf91g","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c9878341-a4a6-4dbb-8876-6c3936a42503"},"source":["stopword = stopwords.words(\"english\")\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"How are your studies going on? \"\n","word_tokens = nltk.word_tokenize(text)\n","lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n","print (lemmatized_word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['How', 'are', 'your', 'study', 'going', 'on', '?']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_7r84SZ_8tBV"},"source":["<a id=section206></a>\n","### **2.6 Stemming**"]},{"cell_type":"markdown","metadata":{"id":"8w4F6DecgX0Q"},"source":["- It is the process of **reducing** inflected (or sometimes derived) words to their word stem, **base** or **root** form."]},{"cell_type":"markdown","metadata":{"id":"nRGKDx50n61M"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nant.JPG\"/></center>"]},{"cell_type":"code","metadata":{"id":"biajxK-BgZFg"},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","#is based on The Porter Stemming Algorithm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXKiFzlDf_W_","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"51246b44-7fda-484c-ef71-b704f804426f"},"source":["stopword = stopwords.words(\"english\")\n","snowball_stemmer = SnowballStemmer(\"english\")\n","text = \"How are your studies going on?\"\n","word_tokens = nltk.word_tokenize(text)\n","stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n","print (stemmed_word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['how', 'are', 'your', 'studi', 'go', 'on', '?']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vULmjhrivaIz"},"source":["#### Lemmatization Vs Stemming?\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/superman-vs-flash-justice-league.png\" width=\"540\" height=\"300\"/></center>\n","\n","- Lemmatization is typically **more accurate** as it uses more informed analysis to create **groups of words** with **similar meaning** based on the content around the world.\n","\n","- While Stemming is **typically faster** as it chops off the end of a word **using heuristics**, **without** any **understanding** of the context in which a word is used apparently.\n","\n","- By applying Stemming we **may or may not** get a **meaningful word.**\n","\n","- By using Lemmatization the words of a sentence will give a **meaningful meaning** of word.\n","\n","- Lemmatization is **accurate** but is **computationally expensive**\n","\n","- Stemming is **not that accurate** but it is **faster.**"]},{"cell_type":"markdown","metadata":{"id":"7LRbZoWA9E6m"},"source":["<a id=section207></a>\n","### **2.7 Get word frequency**"]},{"cell_type":"markdown","metadata":{"id":"3r7A7w6Ygu0X"},"source":["- Through this we convert the **text into vector models** which is based on the occurrence of words in document.\n","\n","- Counting the **word occurrence** using **FreqDist library**."]},{"cell_type":"code","metadata":{"id":"etuWF5lUgxlA","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"47bdd274-7fb0-48c2-b37a-155ca53a4df3"},"source":["import nltk\n","from nltk import FreqDist\n","text = \"Full form of NLTK is Natural Language Toolkit\"\n","word = nltk.word_tokenize(text.lower())\n","freq = FreqDist(word)\n","print (freq.most_common(5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('full', 1), ('form', 1), ('of', 1), ('nltk', 1), ('is', 1)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZBeGtmnBt8zD"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/frequencyhighwords.jpg\" width=\"340\" height=\"200\"></center>\n"]},{"cell_type":"markdown","metadata":{"id":"nxATTgz69IwZ"},"source":["<a id=section208></a>\n","### **2.8  POS(Part of Speech)tags**"]},{"cell_type":"markdown","metadata":{"id":"xMLbnrN5g8Rf"},"source":["- It helps us to know the tags of **each word** like whether a **word is noun, adjective** etc."]},{"cell_type":"markdown","metadata":{"id":"I-iUddF_0grR"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ner.JPG\"width=\"440\" height=\"200\"/></center>"]},{"cell_type":"code","metadata":{"id":"1cbvMXc9htyF","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"bd7bf19e-418d-4c81-98a3-44945dee398f"},"source":["nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"4MRb5-FvhdVC","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24e33be4-4b8c-40eb-ca3a-1535b8a893ae"},"source":["import nltk\n","text = \"the dogs are barking outside.\"\n","word = nltk.word_tokenize(text)\n","pos_tag = nltk.pos_tag(word)\n","print (pos_tag)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GEQ1LZoAbeN8"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/partofspeechtags.JPG\" width=\"640\" height=\"380\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"l9f3BlrD9QXT"},"source":["<a id=section209></a>\n","### 2.9  NER(Named Entity Recognition)"]},{"cell_type":"markdown","metadata":{"id":"zt-vX6ulhJjQ"},"source":["- It is the process of getting the **entity names**.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/entity2.JPG\" width=\"590\" height=\"480\"/></center>\n","\n","- It is the subtask of **information extraction** that classify named entities into predefined categories such as **name of person, organization, location** etc."]},{"cell_type":"code","metadata":{"id":"EKfc9ykhh2Il","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"a88875ab-84d3-4a13-e274-5b5daff4a364"},"source":["import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"SQXndvSlujGo"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/named.JPG\" width=\"420\" height=\"280\"/></center>"]},{"cell_type":"code","metadata":{"id":"F-UV0Z_gijE4","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"089dc12d-4300-42db-ea00-87f172bef34e"},"source":["text = \"Narendar Modi is the Prime Minister of India. Ananta is working at Insaid on Datascience pipelines\"\n","word = nltk.word_tokenize(text)\n","pos_tag = nltk.pos_tag(word)\n","chunk = nltk.ne_chunk(pos_tag)\n","NE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n","print (NE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Narendar', 'Modi', 'India', 'Ananta', 'Insaid', 'Datascience']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vLHVgG2H1tc2"},"source":["---\n","<a name = Section3></a>\n","# **3. Bag of Words Approach**\n","---"]},{"cell_type":"markdown","metadata":{"_uuid":"38f6a4ac426e4aae919a88080b3e06c691f95b2b","id":"etAGZxFSTIXJ"},"source":["- Most ML algorithms rely on **numerical** data to be fed into them as **input**.\n","\n","- We need a way to **represent** **text** data for machine learning algorithm and the __bag-of-words__ model helps us to **achieve** that task.\n","\n","- It is a way of **extracting features** from the **text** for use in machine learning algorithms.\n","\n","- In this approach, we use the **tokenized** words for each **observation** and find out the **frequency** of each token.\n","\n","- Using a **process** which we will go through now, we can **convert** a collection of documents to a **matrix**, with each document being a row and each word (token) being the **column**, and the corresponding (row, column) **values** being the frequency of **occurrence** of each word or **token** in that document.\n","\n","- Understanding the approach using an example:\n","\n","  - Lets say we have **4 documents** as follows:\n","\n","  - **['Hello, how are you!', 'Win money, win from home.', 'Call me now', 'Hello, Call you tomorrow?']**\n","\n","  - Our objective here is to **convert** this set of **text** to a **frequency** distribution **matrix**, as follows:\n","\n","<br> \n","<img src=\"https://image.ibb.co/casG7U/countvectorizer.png\" alt=\"table\">"]},{"cell_type":"markdown","metadata":{"_uuid":"e7d98fe16bb796f82419fa591605f457b0d52836","id":"vIon1cP0TIXJ"},"source":["- Here as we can see, the **documents** are numbered in the **rows**, and each word is a **column** name, with the **corresponding** value being the **frequency** of that word in the **document**.\n","\n","- Lets break this **down** and see how we can do this **conversion** using a small set of documents.\n","\n","- To **handle** this, we will be using sklearn's **CountVectorizer** method which does the following:\n","\n","  1.  It **tokenizes** the string(separates the string into individual words) and gives an **integer** ID to each **token**.\n","\n","  2. It counts the **occurrence** of each of **those tokens**."]},{"cell_type":"markdown","metadata":{"_uuid":"1b1160ede91a8705d6f2aa30db6a198493f29661","id":"RaFQ_wKLTIXK"},"source":["#### Implementation of Bag of Words Approach"]},{"cell_type":"markdown","metadata":{"_uuid":"c801dcc3d64d88c1336de3b9aad1a3a802f711f1","id":"x-DieQfgTIXK"},"source":["**Step 1: Convert all strings to their lower case form.**"]},{"cell_type":"code","metadata":{"id":"SJlpAAzuY6Y_"},"source":["documents = ['Hello, how are you!',\n","             'Win money, win from home.',\n","             'Call me now.',\n","             'Hello, Call hello you tomorrow?']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"74535171bb8c56beef46d910fc322c8c6f8879ef","id":"ZM3sO-11TIXL","outputId":"f90dec43-5fb2-460e-9e6e-5a3607697272"},"source":["lower_case_documents = []\n","lower_case_documents = [d.lower() for d in documents]\n","print(lower_case_documents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"0c7114eeac1932122a01943d9fcdd52f13e86e80","id":"97LZKHRPTIXO"},"source":["**Step 2: Removing all punctuations**"]},{"cell_type":"code","metadata":{"_uuid":"0385f268056be9d84613fbc3a492d8b4695480bf","id":"0CJLEq-aTIXO","outputId":"f4b7ea24-b0ac-4e5d-fcaa-844f6eafdf12"},"source":["sans_punctuation_documents = []\n","import string\n","\n","for i in lower_case_documents:\n","    sans_punctuation_documents.append(i.translate(str.maketrans(\"\",\"\", string.punctuation)))\n","    \n","sans_punctuation_documents"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello how are you',\n"," 'win money win from home',\n"," 'call me now',\n"," 'hello call hello you tomorrow']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"_uuid":"a1a709cf9e09aa2ea1e67009ad19aaba8475fa6f","id":"gFsuF3VvTIXV"},"source":["\n","**Step 3: Tokenization**"]},{"cell_type":"code","metadata":{"_uuid":"ef412ffa525e15273eaace2aae4e374a8933091c","id":"BtOoSKYQTIXW","outputId":"3296a5ce-91f2-4937-83a5-ccdd7a75f089"},"source":["preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]\n","preprocessed_documents"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['hello', 'how', 'are', 'you'],\n"," ['win', 'money', 'win', 'from', 'home'],\n"," ['call', 'me', 'now'],\n"," ['hello', 'call', 'hello', 'you', 'tomorrow']]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"_uuid":"95630beff35cb61b5fd1744b91ca37e7f6dd1d00","id":"3fyl5RsITIXZ"},"source":["**Step 4: Count frequencies**"]},{"cell_type":"code","metadata":{"_uuid":"bdcd4677b6634fbe7f75c47262903efdb38100d1","id":"TvB-sIPkTIXa","outputId":"6965d9ed-d28e-4eac-eb06-ceca28940b01"},"source":["frequency_list = []\n","import pprint\n","from collections import Counter\n","\n","frequency_list = [Counter(d) for d in preprocessed_documents]\n","pprint.pprint(frequency_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n"," Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n"," Counter({'call': 1, 'me': 1, 'now': 1}),\n"," Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"cc97056c261ef100eb715301ae09a068530791d9","id":"p9rLzdVDTIXe"},"source":["#### Implementing Bag of Words in scikit-learn"]},{"cell_type":"markdown","metadata":{"_uuid":"ea63b5f315ff995b52de58a8805bbe10af9c17d6","id":"zBM6C18LTIXe"},"source":["**Data preprocessing with CountVectorizer()**\n","\n","- In above step, we implemented a version of the **`CountVectorizer()`** method from scratch that entailed **cleaning** our data first.\n","\n","- This cleaning **involved** converting all of our data to **lower case** and removing all **punctuation marks**.\n","\n","- **`CountVectorizer()`** has certain **parameters** which take care of these steps for us. They are:\n","\n"," - **lowercase** = True  \n","        \n","   - The **lowercase** parameter has a **default** value of **True** which **converts** all of our text to its **lower case** form.\n","        \n"," - **token_pattern** = (?u)\\\\b\\\\w\\\\w+\\\\b \n","        \n","    - The token_pattern parameter has a **default** **regular expression** value of (?u)\\\\b\\\\w\\\\w+\\\\b which **ignores** all **punctuation marks** and treats them as **delimiters**, while accepting **alphanumeric** strings of length greater than or equal to **2**, as individual **tokens** or words.\n","        \n"," - **stop_words** \n","    \n","    - The stop_words parameter, if set to **english** will remove all **words** from our document set that **match** a list of English **stop words **which is defined in **scikit-learn**. \n","\n","   - Considering the **size** of our dataset and the fact that we are **dealing** with **SMS messages** and not larger **text sources** like e-mail, we will not be setting this **parameter** value."]},{"cell_type":"code","metadata":{"_uuid":"7ca6a8661c2d14ce311a263c8f5b599f099bcd92","id":"gpWRUEtVTIXf"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vector = CountVectorizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"89698bc13286e251f007438e5143a185874dec8a","id":"Pxljhh5vTIXi","outputId":"32731b1c-9eed-4229-ee0d-9f8f3f8066c4"},"source":["count_vector.fit(documents)\n","count_vector.get_feature_names()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['are',\n"," 'call',\n"," 'from',\n"," 'hello',\n"," 'home',\n"," 'how',\n"," 'me',\n"," 'money',\n"," 'now',\n"," 'tomorrow',\n"," 'win',\n"," 'you']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"_uuid":"7bdd5c5b641355b80c84ae24142f5ded3e4054ba","id":"AR20G-utTIXn","outputId":"c289bcc7-d8c9-40d2-b589-79657fdaedf7"},"source":["doc_array = count_vector.transform(documents).toarray()\n","doc_array"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n","       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n","       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n","       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"_uuid":"770d9a3f5b134e81664eea69eae48a38bb5a03aa","id":"bYjdd5gVTIXr","outputId":"16e9a242-3f7a-4121-8423-d8eb7ffc1412"},"source":["frequency_matrix = pd.DataFrame(doc_array, columns=count_vector.get_feature_names())\n","frequency_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>call</th>\n","      <th>from</th>\n","      <th>hello</th>\n","      <th>home</th>\n","      <th>how</th>\n","      <th>me</th>\n","      <th>money</th>\n","      <th>now</th>\n","      <th>tomorrow</th>\n","      <th>win</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n","0    1     0     0      1     0    1   0      0    0         0    0    1\n","1    0     0     1      0     1    0   0      1    0         0    2    0\n","2    0     1     0      0     0    0   1      0    1         0    0    0\n","3    0     1     0      2     0    0   0      0    0         1    0    1"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"fdKTPHvHTIXv"},"source":["- Lets take the second row of the document, i.e., '__Win money, win from home.__' .\n","\n","- Now by observing the second row of the frequency matrix, you can learn how values of individual element is set in the matrix.\n","\n","\n","|__Word__ | __Frequency__  |\n","| -------|:------      :| \n","|from    | 1            |\n","|home    | 1            |\n","|money   | 1            |\n","|win     | 2            |"]},{"cell_type":"markdown","metadata":{"id":"HTPM3ztxzGm1"},"source":["#### TF-IDF (Term Frequency - Inverse Document Frequency)"]},{"cell_type":"markdown","metadata":{"id":"ht5gJGOMzGm3"},"source":["- <font color = red>TF</font> - It signifies the **occurrence** of the word in a **document**.\n","\n","\n","<img src = \"https://raw.githubusercontent.com/insaid2018/Term-4/master/images/tf.PNG\">\n","\n","- <font color = red>IDF</font> - It signifies the **rarity** of the **word** as the word **occuring** in the document.\n","\n","<img src = \"https://raw.githubusercontent.com/insaid2018/Term-4/master/images/idf.PNG\">\n","\n","- <font color = red>TF-IDF</font> - It is a **measure** used to **evaluate** how important a word is to a **document** in a document corpus.\n","\n","- The importance of word increases **proportionally** to the number of times a word appears in the **document** but is offset by the **frequency** of the word in the **corpus**.\n","\n","<br>  "]},{"cell_type":"markdown","metadata":{"id":"KIhGDwCHbDHe"},"source":["TF-IDF is used mainly because of two reasons:\n","\n","- Suppose we search for \"**the rise of AI**\" on **Google**. It is certain that \"the\" will occur **more** frequently than \"**AI**\", but the relative **importance** of **AI** is **higher**.\n","\n","- In such case, **TF-IDF weighting** negates the effect of **high frequency** words in determining the **importance** of an item."]},{"cell_type":"markdown","metadata":{"id":"Iqkn4tvObJDE"},"source":["**TF-IDF in scikit-learn**"]},{"cell_type":"markdown","metadata":{"id":"AmRDII7JcyAs"},"source":["- We can run the **`TfidfVectorizer`** in a similar manner as the **CountVectorizer**.\n","\n","- We can first fit the **vectorizer** with the text data and then **transform** it.\n","\n","- Or we can use use **`fit_transform`**, to perform fitting and transforming in a **single step**."]},{"cell_type":"code","metadata":{"id":"40zTBJBNbmBq"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"414p-3oibU82"},"source":["tfidf_vectorizer = TfidfVectorizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"7bdd5c5b641355b80c84ae24142f5ded3e4054ba","id":"2TEI2O7PcE1Q","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"6c222336-c051-41a5-d908-3470b1f528d1"},"source":["tfidf_array = tfidf_vectorizer.fit_transform(documents).toarray()\n","tfidf_array"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.55528266, 0.        , 0.        , 0.43779123, 0.        ,\n","        0.55528266, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.43779123],\n","       [0.        , 0.        , 0.37796447, 0.        , 0.37796447,\n","        0.        , 0.        , 0.37796447, 0.        , 0.        ,\n","        0.75592895, 0.        ],\n","       [0.        , 0.48693426, 0.        , 0.        , 0.        ,\n","        0.        , 0.61761437, 0.        , 0.61761437, 0.        ,\n","        0.        , 0.        ],\n","       [0.        , 0.362529  , 0.        , 0.725058  , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.45982207,\n","        0.        , 0.362529  ]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"KsXB6-ulcuzW","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5fc1d9fc-e7ca-4d5d-d649-4503b61645e5"},"source":["print(tfidf_vectorizer.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money', 'now', 'tomorrow', 'win', 'you']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"770d9a3f5b134e81664eea69eae48a38bb5a03aa","id":"5DNHYlxEcE1U","colab":{"base_uri":"https://localhost:8080/","height":166},"outputId":"0ea2ead6-1d81-4310-fa3a-43aaa0ee5cd1"},"source":["tfidf_matrix = pd.DataFrame(tfidf_array, columns=tfidf_vectorizer.get_feature_names())\n","tfidf_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>call</th>\n","      <th>from</th>\n","      <th>hello</th>\n","      <th>home</th>\n","      <th>how</th>\n","      <th>me</th>\n","      <th>money</th>\n","      <th>now</th>\n","      <th>tomorrow</th>\n","      <th>win</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.555283</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.437791</td>\n","      <td>0.000000</td>\n","      <td>0.555283</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.437791</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.377964</td>\n","      <td>0.000000</td>\n","      <td>0.377964</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.377964</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.755929</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.486934</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.617614</td>\n","      <td>0.000000</td>\n","      <td>0.617614</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.362529</td>\n","      <td>0.000000</td>\n","      <td>0.725058</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.459822</td>\n","      <td>0.000000</td>\n","      <td>0.362529</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        are      call      from  ...  tomorrow       win       you\n","0  0.555283  0.000000  0.000000  ...  0.000000  0.000000  0.437791\n","1  0.000000  0.000000  0.377964  ...  0.000000  0.755929  0.000000\n","2  0.000000  0.486934  0.000000  ...  0.000000  0.000000  0.000000\n","3  0.000000  0.362529  0.000000  ...  0.459822  0.000000  0.362529\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"e-PzUpNsdcQS"},"source":["- The output is similar to the **CountVectorizer**.\n","\n","- But, instead of the **counts** of each word in the **document**, we are getting **TF-IDF** values."]},{"cell_type":"markdown","metadata":{"id":"aFZn_QCF1v_K"},"source":["---\n","<a id=section4></a>\n","# **4. Applications**\n","---"]},{"cell_type":"markdown","metadata":{"id":"EBrnlyC5XRVN"},"source":["- We can use NLP on a text review to predict if the **review is a good one** or a **bad one.**\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/sentiment%20analysis.gif\"width=\"440\" height=\"300\"></center>\n","\n","- It can be used  on an **article to predict** some categories of the articles you are trying to segment.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/art.png\"width=\"460\" height=\"200\"></center>\n","\n","\n","- It can be used for certain criteria on which it takes **decision** and **blocks spam**(unwanted mails).\n","\n","<br>  \n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/spam%20filter.gif\" width=\"340\" height=\"240\"></center><br>\n","\n","<br>  \n","-  It can be used as a **chat Bot** that uses AI as **integral part** of business world.\n","\n","<br>  \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/4.gif\" width=\"340\" height=\"200\"></center>"]}]}